{
  "url": "https://voxday.net/2019/12/29/the-troll-wars/",
  "scraped_at": "2025-12-21T03:10:56.539672",
  "title": "The troll wars",
  "date_display": "December 29, 2019",
  "date_iso": "2019-12-29T12:32:05+00:00",
  "date_from_url": "2019-12-29",
  "author": "VD",
  "content_html": "<div class=\"entry-content\"><p>The problem with which we’ve been dealing for the last few years is just a microcosm of <a href=\"https://www.theatlantic.com/technology/archive/2017/03/guys-its-time-for-some-troll-theory/521046/\">a much larger one</a> that has disturbing long-term implications for the future direction of the intersection of technology and law:</p><blockquote><p>The resounding message in the Pew report is this: There’s no way the problem in public discourse is going to solve itself. “Between troll attacks, chilling effects of government surveillance and censorship, etc., the internet is becoming narrower every day,” said Randy Bush, a research fellow at Internet Initiative Japan, in his response to Pew.</p><p>Many of those polled said that we’re now witnessing the emergence of “flame wars and strategic manipulation” that will only get worse. This goes beyond obnoxious comments, or Donald Trump’s tweets, or even targeted harassment. Instead, we’ve entered the realm of “weaponized narrative” as a 21st-century battle space, as the authors of a recent Defense One essay put it. And just like other battle spaces, humans will need to develop specialized technology for the fight ahead.</p><p>Researchers have already used technology to begin to understand what they’re up against. Earlier this month, a team of computer scientists from Stanford University and Cornell University wrote about how they used machine-learning algorithms to forecast whether a person was likely to start trolling. Using their algorithm to analyze a person’s mood and the context of the discussion they were in, the researchers got it right 80 percent of the time.   </p><p>They learned that being in a bad mood makes a person more likely to troll, and that trolling is most frequent late at night (and least frequent in the morning). They also tracked the propensity for trolling behavior to spread. When the first comment in a thread is written by a troll—a nebulous term, but let’s go with it—then it’s twice as likely that additional trolls will chime in compared with a conversation that’s not led by a troll to start, the researchers found. On top of that, the more troll comments there are in a discussion, the more likely it is that participants will start trolling in other, unrelated threads.</p><p>“A single troll comment in a discussion—perhaps written by a person who woke up on the wrong side of the bed—can lead to worse moods among other participants, and even more troll comments elsewhere,” the Stanford and Cornell researchers wrote. “As this negative behavior continues to propagate, trolling can end up becoming the norm in communities if left unchecked.”</p><p>Using technology to understand when and why people troll is essential, but many people agree that the scale of the problem requires technological solutions. Stopping trolls isn’t as simple as creating spaces that prevent anonymity, many of those surveyed told Pew, because doing so also enables “governments and dominant institutions to even more freely employ surveillance tools to monitor citizens, suppress free speech, and shape social debate,” Pew wrote.</p></blockquote><p>We’re already seeing how companies like Facebook and Google have weaponized the concept of “fake news”, and now entire countries are following suit. In what is a crushing refutation of libertarian theory, the Internet and the devolution of what were once civilized anonymous discussion spaces on bulletin boards and CompuServe have clearly demonstrated that Man cannot handle the freedom of a perceived lack of accountability.</p><p>There are deeper philosophical aspects to this, that lend additional clarity to traditional thinking about morality and ethics. Even the most devout atheist should be able to recognize at this point that Man was not made for, nor can he reliably handle, even the perceived absence of a Lawgiver to whom he knows he will be held responsible for his actions.</p></div>",
  "content_text": "The problem with which we’ve been dealing for the last few years is just a microcosm of\na much larger one\nthat has disturbing long-term implications for the future direction of the intersection of technology and law:\nThe resounding message in the Pew report is this: There’s no way the problem in public discourse is going to solve itself. “Between troll attacks, chilling effects of government surveillance and censorship, etc., the internet is becoming narrower every day,” said Randy Bush, a research fellow at Internet Initiative Japan, in his response to Pew.\nMany of those polled said that we’re now witnessing the emergence of “flame wars and strategic manipulation” that will only get worse. This goes beyond obnoxious comments, or Donald Trump’s tweets, or even targeted harassment. Instead, we’ve entered the realm of “weaponized narrative” as a 21st-century battle space, as the authors of a recent Defense One essay put it. And just like other battle spaces, humans will need to develop specialized technology for the fight ahead.\nResearchers have already used technology to begin to understand what they’re up against. Earlier this month, a team of computer scientists from Stanford University and Cornell University wrote about how they used machine-learning algorithms to forecast whether a person was likely to start trolling. Using their algorithm to analyze a person’s mood and the context of the discussion they were in, the researchers got it right 80 percent of the time.\nThey learned that being in a bad mood makes a person more likely to troll, and that trolling is most frequent late at night (and least frequent in the morning). They also tracked the propensity for trolling behavior to spread. When the first comment in a thread is written by a troll—a nebulous term, but let’s go with it—then it’s twice as likely that additional trolls will chime in compared with a conversation that’s not led by a troll to start, the researchers found. On top of that, the more troll comments there are in a discussion, the more likely it is that participants will start trolling in other, unrelated threads.\n“A single troll comment in a discussion—perhaps written by a person who woke up on the wrong side of the bed—can lead to worse moods among other participants, and even more troll comments elsewhere,” the Stanford and Cornell researchers wrote. “As this negative behavior continues to propagate, trolling can end up becoming the norm in communities if left unchecked.”\nUsing technology to understand when and why people troll is essential, but many people agree that the scale of the problem requires technological solutions. Stopping trolls isn’t as simple as creating spaces that prevent anonymity, many of those surveyed told Pew, because doing so also enables “governments and dominant institutions to even more freely employ surveillance tools to monitor citizens, suppress free speech, and shape social debate,” Pew wrote.\nWe’re already seeing how companies like Facebook and Google have weaponized the concept of “fake news”, and now entire countries are following suit. In what is a crushing refutation of libertarian theory, the Internet and the devolution of what were once civilized anonymous discussion spaces on bulletin boards and CompuServe have clearly demonstrated that Man cannot handle the freedom of a perceived lack of accountability.\nThere are deeper philosophical aspects to this, that lend additional clarity to traditional thinking about morality and ethics. Even the most devout atheist should be able to recognize at this point that Man was not made for, nor can he reliably handle, even the perceived absence of a Lawgiver to whom he knows he will be held responsible for his actions.",
  "tags": [
    "technology"
  ],
  "categories": [],
  "sitemap_lastmod": "2019-12-29T12:32:05+00:00"
}