{
  "url": "https://voxday.net/2009/11/25/smoking-howitzer/",
  "scraped_at": "2026-01-08T23:13:07.180294",
  "title": "The smoking howitzer",
  "date_display": "November 25, 2009",
  "date_iso": "2009-11-25T10:28:00+00:00",
  "date_from_url": "2009-11-25",
  "author": "VD",
  "content_html": "<div class=\"entry-content\"><p>As bad as they are, <a href=\"http://www.milobookclub.com/2009/11/20/agwcc-fraud-confirmed/\">the hacked CRU emails</a> are actually turning out to be less damning than <a href=\"http://pajamasmedia.com/blog/climategate-computer-codes-are-the-real-story/\">the comments made by the unfortunate programmer</a> who was saddled with the responsibility for trying to transform the morass of data collected by the climatologists into something that was actually coherent and usable.</p><blockquote><p>This is not good — the existing program produces a serious error when it’s run on what is supposed to be the old, working data. Harry presses on, finding a solution to that bug, going through many more issues as he tried to recreate the results of these runs for the data from 1901 to 1995. Finally he gives up. He has spoken to someone about what should be done:</p><p><i>AGREED APPROACH for cloud (5 Oct 06).<br> For 1901 to 1995 – stay with published data. No clear way to replicate process as undocumented.<br> For 1996 to 2002:<br> 1. convert sun database to pseudo-cloud using the f77 programs;<br> 2. anomalise wrt 96-00 with anomdtb.f;<br> 3. grid using quick_interp_tdm.pro (which will use 6190 norms);<br> 4. calculate (mean9600 – mean6190) for monthly grids, using the published cru_ts_2.0 cloud data;<br> 5. add to gridded data from step 3.<br> This should approximate the correction needed.</br></br></br></br></br></br></br></br></i></p><p>Catch that? They couldn’t recreate the results, so they’re going back to their published data for the first 95 years of the 20th century. Only …</p><p><i>Next problem — which database to use? The one with the normals included is not appropriate (the conversion progs do not look for that line so obviously are not intended to be used on +norm databases).</i></p><p>They still don’t know what to use for the next several years. Harry gives up; it’s easier to write new codes.</p><p><i>22. Right, time to stop pussyfooting around the niceties of Tim’s labyrinthine software suites – let’s have a go at producing CRU TS 3.0! since failing to do that will be the definitive failure of the entire project.</i></p><p>This kind of thing is as fascinating as a soap opera, but I want to know how it comes out. Near the bottom of the file, I find:</p><p><i>I am seriously close to giving up, again. The history of this is so complex that I can’t get far enough into it before by head hurts and I have to stop. Each parameter has a tortuous history of manual and semi-automated interventions that I simply cannot just go back to early versions and run the update prog. I could be throwing away all kinds of corrections – to lat/lons, to WMOs (yes!), and more.</i></p><p>The file peters out, no conclusions. I hope they find this poor guy, and he didn’t hang himself in his rooms or something, because this file is a summary of three years of trying to get this data working. Unsuccessfully.  I think there’s a good reason the CRU didn’t want to give their data to people trying to replicate their work. It’s in such a mess that they can’t replicate their own results.</p></blockquote><p>The appearance of these comments is particularly interesting in how it shows that the so-called “scientists” involved in the Great Global Warming Scam are not only committing blatant scientific fraud, they’re technologically incompetent to boot.  Compare this fiasco with the emulator scene, where old and outdated software from decades ago, which is almost surely more complex than mere temperature data sets, is reliably supported by each new generation of hardware… at zero cost to the taxpayer or anyone else!  The AGW/CC “scientists” are contemptible on several levels; only the completely clueless or totally corrupt would permit these dishonest bumblers any input whatsoever on globally significant matters of climate, economy, or government.</p></div>",
  "content_text": "As bad as they are,\nthe hacked CRU emails\nare actually turning out to be less damning than\nthe comments made by the unfortunate programmer\nwho was saddled with the responsibility for trying to transform the morass of data collected by the climatologists into something that was actually coherent and usable.\nThis is not good — the existing program produces a serious error when it’s run on what is supposed to be the old, working data. Harry presses on, finding a solution to that bug, going through many more issues as he tried to recreate the results of these runs for the data from 1901 to 1995. Finally he gives up. He has spoken to someone about what should be done:\nAGREED APPROACH for cloud (5 Oct 06).\nFor 1901 to 1995 – stay with published data. No clear way to replicate process as undocumented.\nFor 1996 to 2002:\n1. convert sun database to pseudo-cloud using the f77 programs;\n2. anomalise wrt 96-00 with anomdtb.f;\n3. grid using quick_interp_tdm.pro (which will use 6190 norms);\n4. calculate (mean9600 – mean6190) for monthly grids, using the published cru_ts_2.0 cloud data;\n5. add to gridded data from step 3.\nThis should approximate the correction needed.\nCatch that? They couldn’t recreate the results, so they’re going back to their published data for the first 95 years of the 20th century. Only …\nNext problem — which database to use? The one with the normals included is not appropriate (the conversion progs do not look for that line so obviously are not intended to be used on +norm databases).\nThey still don’t know what to use for the next several years. Harry gives up; it’s easier to write new codes.\n22. Right, time to stop pussyfooting around the niceties of Tim’s labyrinthine software suites – let’s have a go at producing CRU TS 3.0! since failing to do that will be the definitive failure of the entire project.\nThis kind of thing is as fascinating as a soap opera, but I want to know how it comes out. Near the bottom of the file, I find:\nI am seriously close to giving up, again. The history of this is so complex that I can’t get far enough into it before by head hurts and I have to stop. Each parameter has a tortuous history of manual and semi-automated interventions that I simply cannot just go back to early versions and run the update prog. I could be throwing away all kinds of corrections – to lat/lons, to WMOs (yes!), and more.\nThe file peters out, no conclusions. I hope they find this poor guy, and he didn’t hang himself in his rooms or something, because this file is a summary of three years of trying to get this data working. Unsuccessfully.  I think there’s a good reason the CRU didn’t want to give their data to people trying to replicate their work. It’s in such a mess that they can’t replicate their own results.\nThe appearance of these comments is particularly interesting in how it shows that the so-called “scientists” involved in the Great Global Warming Scam are not only committing blatant scientific fraud, they’re technologically incompetent to boot.  Compare this fiasco with the emulator scene, where old and outdated software from decades ago, which is almost surely more complex than mere temperature data sets, is reliably supported by each new generation of hardware… at zero cost to the taxpayer or anyone else!  The AGW/CC “scientists” are contemptible on several levels; only the completely clueless or totally corrupt would permit these dishonest bumblers any input whatsoever on globally significant matters of climate, economy, or government.",
  "tags": [
    "AGW/CC",
    "science"
  ],
  "categories": [],
  "sitemap_lastmod": "2009-11-25T10:28:00+00:00"
}