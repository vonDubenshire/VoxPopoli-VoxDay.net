{
  "url": "https://voxday.net/2025/11/24/a-civilizational-collapse-model/",
  "scraped_at": "2026-01-08T20:37:28.759398",
  "title": "A Civilizational Collapse Model",
  "date_display": "November 24, 2025",
  "date_iso": "2025-11-24T19:45:18+00:00",
  "date_from_url": "2025-11-24",
  "author": "VD",
  "content_html": "<div class=\"entry-content\"><p>There is <a href=\"https://alwaysthehorizon.substack.com/p/urban-bugmen-and-ai-model-collapse\">an interesting link suggested</a> between the observed AI model collapse and the apparent link between urban society and the collapse of human fertility.</p><blockquote class=\"wp-block-quote is-layout-flow wp-block-quote-is-layout-flow\"><p>The way neural networks function is that they examine real-world data and then create an average of that data to output. The AI output data resembles real-world data (image generation is an excellent example), but valuable minority data is lost. If model 1 trains on 60% black cats and 40% orange cats, then the output for “cat” is likely to yield closer to 75% black cats and 25% orange cats. If model 2 trains on the output of model 1, and model 3 trains on the output of model 2… then by the time you get to the 5th iteration, there are no more orange cats… and the cats themselves quickly become malformed Chronenburg monstrosities.</p><p>Nature published the original associated article in 2024, and follow-up studies have isolated similar issues. Model collapse appears to be a present danger in data sets saturated with AI-generated content4. Training on AI-generated data causes models to hallucinate, become delusional, and deviate from reality to the point where they’re no longer useful: i.e., Model Collapse…</p><p>The proposed thesis is that neural-network systems, which include AI models, human minds, larger human cultures, and our individual furry little friends, all train on available data. When a child stubs his wee little toe on an errant stone and starts screaming as if he’d caught himself on fire, that’s data he just received and which will be added to his model of reality. The same goes for climbing a tree, playing a video game, watching a YouTube video, sitting in a chair, eating that yucky green salad, etc. The child’s mind (or rather, subsections of his brain) are neural networks that behave similarly to AI neural networks.</p><p>The citation is to an article discussing how AI systems are NOT general purpose, and how they more closely resemble individual regions of a brain, not a brain.</p><p>People use new data as training data to model the outside world, particularly when we are children. In the same way that AI models become delusional and hallucinate when too much AI-generated data is in the training dataset, humans also become delusional when too much human-generated data is in their training dataset.</p><p>This is why milennial midwits can’t understand reality unless you figure out a way to reference Harry Potter when trying to make a point.</p><p>What qualifies as “intake data” for humans is nebulous and consists of basically everything. Thus, analyzing the human experience from an external perspective is difficult. However, we can make some broad-stroke statements about human information intake. When a person watches the Olympics, they’re seeing real people interacting with real-world physics. When a person watches a cartoon, they’re seeing artificial people interacting with unrealistic and inaccurate physics. When a human climbs a tree, they’re absorbing real information about gravity, human fragility, and physical strength. When a human plays a high-realism video game, they’re absorbing information artificially produced by other humans to simulate some aspects of the real physical world. When a human watches a cute anime girl driving tanks around, that human is absorbing wholly artificial information created by other humans.</p></blockquote><p>If there is any truth to the hypothesis, this will have profound implications for what passes for human progress as well as the very concept of modernism. Because it’s already entirely clear that Clown World is collapsing and neither modernism nor postmodernism have anything viable to offer humanity a rational path forward.</p><p><em><a href=\"https://socialgalactic.com/micropost/623c928d-254d-4849-8092-80e5d18ad3f1\">DISCUSS ON SG</a></em></p></div>",
  "content_text": "There is\nan interesting link suggested\nbetween the observed AI model collapse and the apparent link between urban society and the collapse of human fertility.\nThe way neural networks function is that they examine real-world data and then create an average of that data to output. The AI output data resembles real-world data (image generation is an excellent example), but valuable minority data is lost. If model 1 trains on 60% black cats and 40% orange cats, then the output for “cat” is likely to yield closer to 75% black cats and 25% orange cats. If model 2 trains on the output of model 1, and model 3 trains on the output of model 2… then by the time you get to the 5th iteration, there are no more orange cats… and the cats themselves quickly become malformed Chronenburg monstrosities.\nNature published the original associated article in 2024, and follow-up studies have isolated similar issues. Model collapse appears to be a present danger in data sets saturated with AI-generated content4. Training on AI-generated data causes models to hallucinate, become delusional, and deviate from reality to the point where they’re no longer useful: i.e., Model Collapse…\nThe proposed thesis is that neural-network systems, which include AI models, human minds, larger human cultures, and our individual furry little friends, all train on available data. When a child stubs his wee little toe on an errant stone and starts screaming as if he’d caught himself on fire, that’s data he just received and which will be added to his model of reality. The same goes for climbing a tree, playing a video game, watching a YouTube video, sitting in a chair, eating that yucky green salad, etc. The child’s mind (or rather, subsections of his brain) are neural networks that behave similarly to AI neural networks.\nThe citation is to an article discussing how AI systems are NOT general purpose, and how they more closely resemble individual regions of a brain, not a brain.\nPeople use new data as training data to model the outside world, particularly when we are children. In the same way that AI models become delusional and hallucinate when too much AI-generated data is in the training dataset, humans also become delusional when too much human-generated data is in their training dataset.\nThis is why milennial midwits can’t understand reality unless you figure out a way to reference Harry Potter when trying to make a point.\nWhat qualifies as “intake data” for humans is nebulous and consists of basically everything. Thus, analyzing the human experience from an external perspective is difficult. However, we can make some broad-stroke statements about human information intake. When a person watches the Olympics, they’re seeing real people interacting with real-world physics. When a person watches a cartoon, they’re seeing artificial people interacting with unrealistic and inaccurate physics. When a human climbs a tree, they’re absorbing real information about gravity, human fragility, and physical strength. When a human plays a high-realism video game, they’re absorbing information artificially produced by other humans to simulate some aspects of the real physical world. When a human watches a cute anime girl driving tanks around, that human is absorbing wholly artificial information created by other humans.\nIf there is any truth to the hypothesis, this will have profound implications for what passes for human progress as well as the very concept of modernism. Because it’s already entirely clear that Clown World is collapsing and neither modernism nor postmodernism have anything viable to offer humanity a rational path forward.\nDISCUSS ON SG",
  "tags": [
    "decline and fall",
    "philosophy",
    "science",
    "technology"
  ],
  "categories": [],
  "sitemap_lastmod": "2025-11-24T19:45:22+00:00"
}