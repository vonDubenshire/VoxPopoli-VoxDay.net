{
  "url": "https://voxday.net/2016/04/13/the-intrinsic-unreliability-of-science/",
  "scraped_at": "2025-12-17T17:53:35.486310",
  "title": "The intrinsic unreliability of science",
  "date_display": "April 13, 2016",
  "date_iso": "2016-04-13T12:00:00+00:00",
  "date_from_url": "2016-04-13",
  "author": "VD",
  "content_html": "<div class=\"entry-content\"><p>More and more investigations of quasi-scientific shenanigans are demonstrating the need for more precision in the language used to describe <a href=\"http://www.firstthings.com/article/2016/05/scientific-regress\">the field that is too broadly and misleadingly known as “science”</a>:</p><blockquote class=\"tr_bq\"><p> The problem with ­science is that so much of it simply isn’t. Last summer, the Open Science Collaboration announced that it had tried to replicate one hundred published psychology experiments sampled from three of the most prestigious journals in the field. Scientific claims rest on the idea that experiments repeated under nearly identical conditions ought to yield approximately the same results, but until very recently, very few had bothered to check in a systematic way whether this was actually the case. The OSC was the biggest attempt yet to check a field’s results, and the most shocking. In many cases, they had used original experimental materials, and sometimes even performed the experiments under the guidance of the original researchers. Of the studies that had originally reported positive results, an astonishing 65 percent failed to show statistical significance on replication, and many of the remainder showed greatly reduced effect sizes.</p><p>Their findings made the news, and quickly became a club with which to bash the social sciences. But the problem isn’t just with psychology. There’s an ­unspoken rule in the pharmaceutical industry that half of all academic biomedical research will ultimately prove false, and in 2011 a group of researchers at Bayer decided to test it. Looking at sixty-seven recent drug discovery projects based on preclinical cancer biology research, they found that in more than 75 percent of cases the published data did not match up with their in-house attempts to replicate. These were not studies published in fly-by-night oncology journals, but blockbuster research featured in Science, Nature, Cell, and the like. The Bayer researchers were drowning in bad studies, and it was to this, in part, that they attributed the mysteriously declining yields of drug pipelines. Perhaps so many of these new drugs fail to have an effect because the basic research on which their development was based isn’t valid….</p><div class=\"p2\"> Paradoxically, the situation is actually made worse by the<br> fact that a promising connection is often studied by several<br> independent teams. To see why, suppose that three groups of researchers<br> are studying a phenomenon, and when all the data are analyzed, one group<br> announces that it has discovered a connection, but the other two find<br> nothing of note. Assuming that all the tests involved have a high<br> statistical power, the lone positive finding is almost certainly the<br> spurious one. However, when it comes time to report these findings, what<br> happens? The teams that found a negative result may not even bother to<br> write up their non-discovery. After all, a report that a fanciful<br> connection probably isn’t true is not the stuff of which scientific<br> prizes, grant money, and tenure decisions are made.</br></br></br></br></br></br></br></br></br></br></br></div><div class=\"p2\"></div><div class=\"p2\"> And even if they did write it up, it probably wouldn’t be<br> accepted for publication. Journals are in competition with one another<br> for attention and “impact factor,” and are always more eager to report a<br> new, exciting finding than a killjoy failure to find an association. In<br> fact, both of these effects can be quantified. Since the majority of<br> all investigated hypotheses are false, if positive and negative evidence<br> were written up and accepted for publication in equal proportions, then<br> the majority of articles in scientific journals should report no<br> findings. When tallies are actually made, though, the precise opposite<br> turns out to be true: Nearly every published scientific article reports<br> the presence of an association. There must be massive bias at work. </br></br></br></br></br></br></br></br></br></br></div><div class=\"p2\"></div><div class=\"p3\"> Ioannidis’s argument would be potent even if all<br> scientists were angels motivated by the best of intentions, but when the<br> human element is considered, the picture becomes truly dismal.<br> Scientists have long been aware of something euphemistically called the<br> “experimenter effect”: the curious fact that when a phenomenon is<br> investigated by a researcher who happens to believe in the phenomenon,<br> it is far more likely to be detected. Much of the effect can likely be<br> explained by researchers unconsciously giving hints or suggestions to<br> their human or animal subjects, perhaps in something as subtle as body<br> language or tone of voice. Even those with the best of intentions have<br> been caught fudging measurements, or making small errors in rounding or<br> in statistical analysis that happen to give a more favorable result.<br> Very often, this is just the result of an honest statistical error that<br> leads to a desirable outcome, and therefore it isn’t checked as<br> deliberately as it might have been had it pointed in the opposite<br> direction. </br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></div><div class=\"p3\"></div><p>But, and there is no putting it nicely, deliberate fraud<br> is far more widespread than the scientific establishment is generally<br> willing to admit.</br></br></p></blockquote><p>Never confuse either scientistry or sciensophy for scientody. To paraphrase, and reject, Daniel Dennett’s contention, do not trust biologists or sociologists or climatologists, or anyone else who calls himself a scientist, simply because physicists get amazingly accurate results.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"> <a href=\"https://3.bp.blogspot.com/-2Rw-98BO3J0/SXZHIRt_oKI/AAAAAAAAA2c/BT04G1WTj6c0tw55Qzo1gQugIYikxnnug/s1600/ATHEIST%2BLOGIC.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" decoding=\"async\" fetchpriority=\"high\" height=\"320\" src=\"http://www.milobookclub.com/wp-content/uploads/2016/04/ATHEISTLOGIC.jpg\" width=\"256\"/></a></div><p></p></div>",
  "content_text": "More and more investigations of quasi-scientific shenanigans are demonstrating the need for more precision in the language used to describe\nthe field that is too broadly and misleadingly known as “science”\n:\nThe problem with ­science is that so much of it simply isn’t. Last summer, the Open Science Collaboration announced that it had tried to replicate one hundred published psychology experiments sampled from three of the most prestigious journals in the field. Scientific claims rest on the idea that experiments repeated under nearly identical conditions ought to yield approximately the same results, but until very recently, very few had bothered to check in a systematic way whether this was actually the case. The OSC was the biggest attempt yet to check a field’s results, and the most shocking. In many cases, they had used original experimental materials, and sometimes even performed the experiments under the guidance of the original researchers. Of the studies that had originally reported positive results, an astonishing 65 percent failed to show statistical significance on replication, and many of the remainder showed greatly reduced effect sizes.\nTheir findings made the news, and quickly became a club with which to bash the social sciences. But the problem isn’t just with psychology. There’s an ­unspoken rule in the pharmaceutical industry that half of all academic biomedical research will ultimately prove false, and in 2011 a group of researchers at Bayer decided to test it. Looking at sixty-seven recent drug discovery projects based on preclinical cancer biology research, they found that in more than 75 percent of cases the published data did not match up with their in-house attempts to replicate. These were not studies published in fly-by-night oncology journals, but blockbuster research featured in Science, Nature, Cell, and the like. The Bayer researchers were drowning in bad studies, and it was to this, in part, that they attributed the mysteriously declining yields of drug pipelines. Perhaps so many of these new drugs fail to have an effect because the basic research on which their development was based isn’t valid….\nParadoxically, the situation is actually made worse by the\nfact that a promising connection is often studied by several\nindependent teams. To see why, suppose that three groups of researchers\nare studying a phenomenon, and when all the data are analyzed, one group\nannounces that it has discovered a connection, but the other two find\nnothing of note. Assuming that all the tests involved have a high\nstatistical power, the lone positive finding is almost certainly the\nspurious one. However, when it comes time to report these findings, what\nhappens? The teams that found a negative result may not even bother to\nwrite up their non-discovery. After all, a report that a fanciful\nconnection probably isn’t true is not the stuff of which scientific\nprizes, grant money, and tenure decisions are made.\nAnd even if they did write it up, it probably wouldn’t be\naccepted for publication. Journals are in competition with one another\nfor attention and “impact factor,” and are always more eager to report a\nnew, exciting finding than a killjoy failure to find an association. In\nfact, both of these effects can be quantified. Since the majority of\nall investigated hypotheses are false, if positive and negative evidence\nwere written up and accepted for publication in equal proportions, then\nthe majority of articles in scientific journals should report no\nfindings. When tallies are actually made, though, the precise opposite\nturns out to be true: Nearly every published scientific article reports\nthe presence of an association. There must be massive bias at work.\nIoannidis’s argument would be potent even if all\nscientists were angels motivated by the best of intentions, but when the\nhuman element is considered, the picture becomes truly dismal.\nScientists have long been aware of something euphemistically called the\n“experimenter effect”: the curious fact that when a phenomenon is\ninvestigated by a researcher who happens to believe in the phenomenon,\nit is far more likely to be detected. Much of the effect can likely be\nexplained by researchers unconsciously giving hints or suggestions to\ntheir human or animal subjects, perhaps in something as subtle as body\nlanguage or tone of voice. Even those with the best of intentions have\nbeen caught fudging measurements, or making small errors in rounding or\nin statistical analysis that happen to give a more favorable result.\nVery often, this is just the result of an honest statistical error that\nleads to a desirable outcome, and therefore it isn’t checked as\ndeliberately as it might have been had it pointed in the opposite\ndirection.\nBut, and there is no putting it nicely, deliberate fraud\nis far more widespread than the scientific establishment is generally\nwilling to admit.\nNever confuse either scientistry or sciensophy for scientody. To paraphrase, and reject, Daniel Dennett’s contention, do not trust biologists or sociologists or climatologists, or anyone else who calls himself a scientist, simply because physicists get amazingly accurate results.",
  "tags": [
    "conspiracy",
    "science"
  ],
  "categories": [],
  "sitemap_lastmod": "2016-04-13T11:00:00+00:00"
}