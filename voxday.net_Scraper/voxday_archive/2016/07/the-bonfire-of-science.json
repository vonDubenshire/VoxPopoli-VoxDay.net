{
  "url": "https://voxday.net/2016/07/05/the-bonfire-of-science/",
  "scraped_at": "2025-12-18T01:42:52.628153",
  "title": "The bonfire of science",
  "date_display": "July 5, 2016",
  "date_iso": "2016-07-05T08:50:00+00:00",
  "date_from_url": "2016-07-05",
  "author": "VD",
  "content_html": "<div class=\"entry-content\"><p>In which it is once more demonstrated that <a href=\"http://www.theregister.co.uk/2016/07/03/mri_software_bugs_could_upend_years_of_research/\">scientific evidence is VASTLY less reliable</a> than other types of evidence, because, in most cases, no one ever bothers to actually check the results:</p><blockquote class=\"tr_bq\"><p> A whole pile of “this is how your brain looks like” MRI-based science has been invalidated because someone finally got around to checking the data.</p><p>The problem is simple: to get from a high-resolution magnetic resonance imaging scan of the brain to a scientific conclusion, the brain is divided into tiny “voxels”. Software, rather than humans, then scans the voxels looking for clusters.</p><p>When you see a claim that “scientists know when you’re about to move an arm: these images prove it”, they’re interpreting what they’re told by the statistical software.</p><p>Now, boffins from Sweden and the UK have cast doubt on the quality of the science, because of problems with the statistical software: it produces way too many false positives.</p><p>In this paper at PNAS, they write: “the most common software packages for fMRI analysis (SPM, FSL, AFNI) can result in false-positive rates of up to 70%. These results question the validity of some 40,000 fMRI studies and may have a large impact on the interpretation of neuroimaging results.”</p><p>For example, a bug that’s been sitting in a package called 3dClustSim for 15 years, fixed in May 2015, produced bad results (3dClustSim is part of the AFNI suite; the others are SPM and FSL).</p><p>That’s not a gentle nudge that some results might be overstated: it’s more like making a bonfire of thousands of scientific papers.</p></blockquote><p>It is not even remotely reasonable to take scientific evidence at face value anymore, much less the pseudoscience so often being substituted for the results produced by genuine, if often flawed, scientody.</p><p>It is not even remotely surprising that the flaw the scientists failed to pick up in this situation was statistical, for <a href=\"http://www.milobookclub.com/2010/03/05/when-science-is-no-longer-science/\">as I’ve previously observed</a>, most scientists have very little training in math or statistics, and despite their habit of regularly citing statistics, most of them are more or less statistically illiterate.</p><p>Never forget that while there are certainly some brilliant scientists, most of them are literal midwits as there are relatively few credentialed scientists with IQs over 132. A study of all the U.S. PhD recipients in 1958 reported an average IQ of 123; the Flynn effect notwithstanding, it is highly unlikely that the average IQ of today’s increasingly diverse and vibrant PhD recipients has risen since then.</p></div>",
  "content_text": "In which it is once more demonstrated that\nscientific evidence is VASTLY less reliable\nthan other types of evidence, because, in most cases, no one ever bothers to actually check the results:\nA whole pile of “this is how your brain looks like” MRI-based science has been invalidated because someone finally got around to checking the data.\nThe problem is simple: to get from a high-resolution magnetic resonance imaging scan of the brain to a scientific conclusion, the brain is divided into tiny “voxels”. Software, rather than humans, then scans the voxels looking for clusters.\nWhen you see a claim that “scientists know when you’re about to move an arm: these images prove it”, they’re interpreting what they’re told by the statistical software.\nNow, boffins from Sweden and the UK have cast doubt on the quality of the science, because of problems with the statistical software: it produces way too many false positives.\nIn this paper at PNAS, they write: “the most common software packages for fMRI analysis (SPM, FSL, AFNI) can result in false-positive rates of up to 70%. These results question the validity of some 40,000 fMRI studies and may have a large impact on the interpretation of neuroimaging results.”\nFor example, a bug that’s been sitting in a package called 3dClustSim for 15 years, fixed in May 2015, produced bad results (3dClustSim is part of the AFNI suite; the others are SPM and FSL).\nThat’s not a gentle nudge that some results might be overstated: it’s more like making a bonfire of thousands of scientific papers.\nIt is not even remotely reasonable to take scientific evidence at face value anymore, much less the pseudoscience so often being substituted for the results produced by genuine, if often flawed, scientody.\nIt is not even remotely surprising that the flaw the scientists failed to pick up in this situation was statistical, for\nas I’ve previously observed\n, most scientists have very little training in math or statistics, and despite their habit of regularly citing statistics, most of them are more or less statistically illiterate.\nNever forget that while there are certainly some brilliant scientists, most of them are literal midwits as there are relatively few credentialed scientists with IQs over 132. A study of all the U.S. PhD recipients in 1958 reported an average IQ of 123; the Flynn effect notwithstanding, it is highly unlikely that the average IQ of today’s increasingly diverse and vibrant PhD recipients has risen since then.",
  "tags": [
    "science",
    "trainwreck"
  ],
  "categories": [],
  "sitemap_lastmod": "2016-07-05T07:50:00+00:00"
}